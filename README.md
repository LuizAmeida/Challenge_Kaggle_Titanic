# üö¢ Challenge Kaggle: An√°lise Preditiva do Titanic

![Kaggle](https://img.shields.io/badge/Kaggle-Titanic-blue.svg)
![Python](https://img.shields.io/badge/Python-3.10%2B-brightgreen.svg)
![Pandas](https://img.shields.io/badge/Pandas-2.x-blueviolet.svg)
![Scikit-Learn](https://img.shields.io/badge/Scikit--Learn-1.x-orange.svg)

---

## üéØ Vis√£o Geral do Projeto

Este reposit√≥rio cont√©m a solu√ß√£o completa para o desafio "Titanic: Machine Learning from Disaster" do Kaggle. O objetivo principal √© construir um modelo de machine learning que prev√™ a sobreviv√™ncia de passageiros a bordo do RMS Titanic. 

Este projeto documenta o fluxo de trabalho de ponta a ponta, desde a an√°lise explorat√≥ria e limpeza dos dados at√© a engenharia de atributos, treinamento, otimiza√ß√£o de modelos e submiss√£o final. O trabalho resultou em uma pontua√ß√£o de **0.77990**, posicionando a submiss√£o entre os **top 20-25%** dos competidores.

---

## üßä O Desafio

O naufr√°gio do RMS Titanic √© um dos eventos mais not√≥rios da hist√≥ria. Em 15 de abril de 1912, durante sua viagem inaugural, o navio colidiu com um iceberg e afundou, resultando na morte de 1502 dos 2224 passageiros e tripulantes.

O desafio consiste em usar um conjunto de dados contendo informa√ß√µes sobre 891 passageiros (dados de treino) para construir um modelo preditivo. Em seguida, esse modelo √© usado para prever o destino de outros 418 passageiros (dados de teste).

---

## üìÅ Estrutura do Reposit√≥rio
- **`Challenge_Kaggle_Titanic.ipynb`**: O notebook Jupyter contendo todo o c√≥digo, an√°lises e a narrativa do desenvolvimento do projeto, do in√≠cio ao fim.
- **`submission_*.csv`**: Pasta contendo os diferentes arquivos de submiss√£o gerados durante o ciclo de experimenta√ß√£o. O arquivo correspondente √† melhor pontua√ß√£o foi o `submissao_com_bins.csv`.

---

## üó∫Ô∏è Metodologia e Fluxo de Trabalho

O projeto foi desenvolvido de forma iterativa, seguindo as melhores pr√°ticas de um pipeline de ci√™ncia de dados.

### ‚úîÔ∏è 1. An√°lise Explorat√≥ria e Limpeza de Dados
A primeira etapa consistiu em uma imers√£o nos dados para entender sua estrutura e identificar problemas.
- **Identifica√ß√£o de Nulos:** Constatou-se a aus√™ncia de dados significativos nas colunas `Age` (Idade) e `Cabin` (Cabine).
- **Tratamento Estrat√©gico:**
    - `Age`: Os valores ausentes foram preenchidos com a **mediana**, uma medida robusta a outliers.
    - `Embarked`: Os poucos valores ausentes foram preenchidos com a **moda** (o porto de embarque mais comum).
    - `Cabin`: Devido ao alto volume de dados faltantes (>75%), a coluna foi inicialmente descartada para simplificar o modelo base.

### ‚ú® 2. Engenharia de Atributos (Feature Engineering)
Esta foi a etapa mais crucial para o sucesso do modelo. V√°rias features novas e informativas foram criadas para enriquecer o dataset e fornecer mais "sinais" para o algoritmo.
- **T√≠tulo (Title):** Extra√≠do da coluna `Name` (ex: `Mr.`, `Mrs.`, `Master.`), servindo como um forte indicador de status social, idade e sexo. T√≠tulos raros foram agrupados para evitar ru√≠do.
- **Tamanho da Fam√≠lia (FamilySize) e Est√° Sozinho (IsAlone):** Criadas a partir da soma de `SibSp` (irm√£os/c√¥njuges) e `Parch` (pais/filhos). Isso permitiu capturar o efeito do tamanho do grupo familiar na sobreviv√™ncia.
- **Faixas de Idade e Tarifa (Binning):** As vari√°veis cont√≠nuas `Age` e `Fare` foram agrupadas em categorias (ex: "Crian√ßa", "Adulto"; "Tarifa Baixa", "Tarifa Alta"). Esta t√©cnica ajuda modelos lineares a capturarem rela√ß√µes n√£o-lineares de forma mais eficaz.
- **Transforma√ß√£o Categ√≥rica:** Colunas textuais como `Sex` e `Embarked` foram convertidas para um formato num√©rico atrav√©s de mapeamento e One-Hot Encoding.

### üß† 3. Modelagem e Experimenta√ß√£o
Uma abordagem sistem√°tica foi utilizada para selecionar o melhor modelo.
- **Baseline Inicial:** Um modelo de **Regress√£o Log√≠stica** foi treinado com os dados b√°sicos, alcan√ßando um score de **0.76794**.
- **Competi√ß√£o de Modelos:** V√°rios algoritmos (`Random Forest`, `SVM`, `Decision Tree`) foram comparados. A Regress√£o Log√≠stica se manteve como a mais est√°vel e com melhor performance inicial.
- **Diagn√≥stico de Overfitting:** O `Random Forest`, em suas configura√ß√µes padr√£o, demonstrou ser propenso a overfitting neste dataset, resultando em pontua√ß√µes mais baixas no conjunto de teste. Esta foi uma descoberta chave do projeto.
- **T√©cnicas Avan√ßadas:** Foram realizadas tentativas com `GridSearchCV` para otimizar o Random Forest e com `VotingClassifier` (Ensemble), mas os resultados n√£o superaram o modelo de Regress√£o Log√≠stica alimentado com as features bem constru√≠das.

---

## üèÜ Resultados e Conclus√£o Final

O projeto culminou em m√∫ltiplas submiss√µes, onde cada itera√ß√£o testava uma nova hip√≥tese.

- **Melhor Pontua√ß√£o:** **0.77990**
- **Posi√ß√£o no Ranking:** **#3058** (entre dezenas de milhares de equipes)
- **Modelo Campe√£o:** **Regress√£o Log√≠stica**
- **Features Decisivas:** A combina√ß√£o de `Title`, `FamilySize`, `IsAlone`, e os *bins* de `Age` e `Fare`.

üí° A principal conclus√£o deste projeto √© que uma **engenharia de atributos cuidadosa e inteligente foi mais impactante do que a complexidade do modelo**. A capacidade de extrair e moldar informa√ß√µes relevantes dos dados brutos foi o verdadeiro diferencial para construir um modelo preditivo robusto e eficaz.

---

## üõ†Ô∏è Tecnologias Utilizadas
- **Linguagem:** Python 3
- **Bibliotecas:**
    - `Pandas` para manipula√ß√£o e an√°lise de dados.
    - `Numpy` para opera√ß√µes num√©ricas.
    - `Scikit-learn` para pr√©-processamento, modelagem e avalia√ß√£o.
- **Ambiente:** Google Colab

---

## üöÄ Como Executar
1.  Clone este reposit√≥rio.
2.  Fa√ßa o upload do notebook (`Challenge_Kaggle_Titanic.ipynb`) para o Google Colab ou abra-o em um ambiente Jupyter local.
3.  Certifique-se de que os arquivos `train.csv` e `test.csv` estejam no caminho especificado no notebook.
4.  Execute as c√©lulas em sequ√™ncia para reproduzir a an√°lise e os resultados.

---

## üë®‚Äçüíª Autor
- **Luiz Marques**
